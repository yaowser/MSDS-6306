---
title: "cybercoders"
author: "Yao Yao"
date: "March 16, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Data Science Profiles Analysis

#Introduction

CyberCoders is a website for technical job postings. We will user R to understand the market for data science based on the job postings on <https://www.cybercoders.com>

#Data Collection

We will use XML (<https://cran.r-project.org/web/packages/XML/index.html>). First, we will write functions to parse the website and the collect the required data. Following chunk of code shows how to search on the website using R. *RCurl::getForm* submits our search query to the website. Also, after taking a look at the html code for the website (*View page source* in the browser), let's also extract the hyperlinks for each of the job postings on the webpage.

Load the libraries that are used for data collection.
```{r}
library(XML)
library(RCurl)
library(tm)
library(wordcloud)
```

Submit search query to website

```{r}
# Extract the search web page
txt = getForm("https://www.cybercoders.com/search/", searchterms = '"Data Scientist"',
              searchlocation = "",  newsearch = "true", sorttype = "")
doc <- htmlParse(txt, asText = TRUE)
# Extract links for each job posting
links <- getNodeSet(doc, "//div[@class = 'job-title']/a/@href")
joblinks <- getRelativeURL(as.character(links), "https://www.cybercoders.com/search/")

print(length(joblinks))
```

There are 20 job postings on the first web page for the query of Data Scientist. After looking the page source for the job posting, we can extract the information of the posting like the job description, posting date, skills required and salary range. Let's write seperate functions to extract each of these and test them.

#Extract attributes of job posting

First, let's extract the job description and test it with the first link in our *joblinks* variable. The function will extract the text, remove punctuations, convert to lower case and remove stop words. It will return a list of words that are present in the text. This will be useful in understanding the key words used by job posters who are looking for Data Scientists.

```{r}
# Remove stop words
removeStopWords = function(x, stopWords = stopwords(kind='en')) 
{
  if(is.character(x))
    setdiff(x, stopWords)
  else if(is.list(x))
    lapply(x, removeStopWords, stopWords)
  else
    x
}


# Extract text of job description
cy.getJobWords = function(doc)
{
  nodes = getNodeSet(doc, "//div[@class='job-details']/
                     div[@data-section]") # Match node criteria and extract corresponding text
  
  if(length(nodes) == 0) 
    nodes = getNodeSet(doc, "//div[@class='job-details']//p")
  
  if(length(nodes) == 0) 
    warning("did not find any nodes for the free form text in ",
            docName(doc))
  # Split the text into individual words and remove punctuations
  words = lapply(nodes,
                 function(x)strsplit(xmlValue(x), "[[:space:][:punct:]]+"))
  words_lower <- removeNumbers(tolower(unlist(words))) # Conver to lower case and remove numbers
  return(removeStopWords(words_lower))
}

job_doc <- htmlParse(getURLContent(joblinks[1])) # HTML page
jd <- cy.getJobWords(doc=job_doc)
jd
```

Similarly, the following function extracts the preferred skill set listed by the job poster.

```{r}
# Extract skill set
cy.getSkillList = function(doc)
{
  lis = getNodeSet(doc, "//div[@class = 'skills-section']//
                   li[@class = 'skill-item']//
                   span[@class = 'skill-name']")
  
  sapply(lis, xmlValue)
}
skill_set <- cy.getSkillList(job_doc)
skill_set
```

The following function extracts the basic job information like location, salary range and the date of the posting.

```{r}
# Trim leading and trailing white spaces
trim <- function (x) gsub("^\\s+|\\s+$", "", x)

# Extract location and salary
cy.getLocationSalary = function(doc)
{
  ans <- xpathSApply(doc, "//div[@class = 'job-info-main'][1]/div",xmlValue)
  ans <- lapply(ans, trim)
  loc <- ans[[1]]
  salary <- ans[[2]]
  post_date <-  as.Date(strsplit(ans[[3]], " ")[[1]][2], format = "%m/%d/%Y")
  job_info <- list(loc, salary, post_date)
  names(job_info) <- c("Location", "Salary", "Post_Date")
  return(job_info)
}

job_info <- cy.getLocationSalary(job_doc)
job_info
```

Great! Now we can use this functions to extract all the job postings on one web page.

```{r}
# Extract the post
cy.readPost = function(u, stopWords = StopWords, doc = htmlParse(getURLContent(u)))
{
  ans = list(words = cy.getJobWords(doc),
             skills = cy.getSkillList(doc))
  o = cy.getLocationSalary(doc)
  ans[names(o)] = o
  ans
}

posts <- lapply(joblinks,cy.readPost)
posts[1]
```

*posts* variable is a named list which holds information about each of the job postings on first web page of the query. The following function puts all of the above steps in one function.

```{r}
cy.getPostLinks = function(doc, baseURL = "https://www.cybercoders.com/search/") 
{
  if(is.character(doc)) doc = htmlParse(doc)
  links = getNodeSet(doc, "//div[@class = 'job-title']/a/@href") 
  getRelativeURL(as.character(links), baseURL)
}

cy.readPagePosts = function(doc, links = cy.getPostLinks(doc, baseURL),baseURL = "https://www.cybercoders.com/search/")
{
  if(is.character(doc)) doc = htmlParse(doc)
  lapply(links, cy.readPost)
}

## Testing the function with the parsed version of the first page of results in object doc
posts = cy.readPagePosts(doc)
sapply(posts,`[[`, "Salary")
```

Now, we would like to go the next page of the search query and extract information about all of the job postings. Following function facilitates navigating to the next page and the above functions could then be used to extract the data.

```{r}
## A function to get all pages
cy.getNextPageLink = function(doc, baseURL = docName(doc))
{
  if(is.na(baseURL))
    baseURL = "https://www.cybercoders.com/"
  link = getNodeSet(doc, "//li[@class = 'lnk-next pager-item ']/a/@href")
  if(length(link) == 0)
    return(character())
  link2 <- gsub("./", "search/",link[[1]])
  getRelativeURL(link2, baseURL)
}

# Test the above function
tmp = cy.getNextPageLink(doc, "http://www.cybercoders.com")
tmp
```

#All in One!

The following function combines all the functions together to automatically extract all the job postings from the website and store it in the desired format.

```{r}
cyberCoders =
  function(query)
  {
    txt = getForm("https://www.cybercoders.com/search/",
                  searchterms = query,  searchlocation = "",
                  newsearch = "true",  sorttype = "")
    doc = htmlParse(txt)
    
    posts = list()
    while(TRUE) {
      posts = c(posts, cy.readPagePosts(doc))
      nextPage = cy.getNextPageLink(doc)
      if(length(nextPage) == 0)
        break
      
      nextPage = getURLContent(nextPage)
      doc = htmlParse(nextPage, asText = TRUE)
    }
    invisible(posts)
  }

dataScience <- cyberCoders("Data Scienctist")
skillSet = sort(table(unlist(lapply(dataScience, `[[`, "skills"))),
                decreasing = TRUE)
skillSet[skillSet >= 2]
```

#Analysis - Skills

A simple way to visualize all the skills with respect to their frequency is to use a word cloud.

```{r}
wordcloud(names(skillSet),skillSet,scale = c(3,1),max.words = 25,colors=brewer.pal(8, "Dark2"))
```
